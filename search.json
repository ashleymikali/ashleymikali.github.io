[
  {
    "objectID": "assignments/job1/coverletter1.html",
    "href": "assignments/job1/coverletter1.html",
    "title": "Cover Letter",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "assignments/job1/jobapp1.html",
    "href": "assignments/job1/jobapp1.html",
    "title": "Job Application 1",
    "section": "",
    "text": "Included are my resume, cover letter and project! Please note that the project is currently undergoing an overhaul and therefore the contents of the project are outdated and are not accurately reflected by the resume. The updated version will be posted soon.\n\n\n\n Back to top"
  },
  {
    "objectID": "placeholder.html",
    "href": "placeholder.html",
    "title": "This, my friend, is what we call a ‚ú® placeholder page ‚ú®",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Projects",
      "Onion Article Classification"
    ]
  },
  {
    "objectID": "garden/crab-cult/crab-cult.html",
    "href": "garden/crab-cult/crab-cult.html",
    "title": "Maybe I, Too, Am a Member of the Crustacean Cult",
    "section": "",
    "text": "I did oral exams disguised with a mustache and a hat mock job interviews in my Statistical Machine Learning class this past semester. I listed both Python and R (along with relevant libraries and packages) on my resume, and one question I had NOT considered before came up towards the start of my first interview.\nI‚Äôm paraphrasing here, but the gist is: ‚ÄúI see that you have both Python and R on your resume‚Ä¶ which do you prefer, and why?‚Äù\nI think I deserve a silver medal in bullshitting. I‚Äôm a modest person, so I can acknowledge that my performance wasn‚Äôt worthy of a gold medal.\nBut anyways, I was only half bullshitting with my answer. And even then, I wasn‚Äôt really bullshitting. I did answer to the best of my abilities. But man, did it feel like I was bullshitting my teacher (if you‚Äôre reading this, I‚Äôm sorry).\nI gave the only answer I could conceive of: ‚ÄúBecause R feels easier to me.‚Äù\nI walked right into a trap with that one. Notice I didn‚Äôt say better (which would have given me so much ammo for bullshitting), rather, I said easier. My teacher hit me back with the ‚Äúbut why?‚Äù And, I mean, I can‚Äôt blame him for that. He made valid points: Python is a beginner-friendly language, and most people consider it to be more straightforward and intuitive. Naturally, most students in my class would likely say scikit-learn &gt; tidymodels (I promise this is NOT an ‚ÄúI‚Äôm not like other programmers!‚Äù article).\nOh, did I mention that I haven‚Äôt written anything computational-statistics-wise with Python in years? My last (and first, and only) experience in this domain was with a Statistics for Data Science with Python MOOC I took on Coursera in‚Ä¶ 2022 (late 2021, really), along with some tinkering with scikit-learn I did shortly thereafter.\nNow, now, you just have to believe me on that one. I promise I‚Äôm not exercising my bullshitting skills again right now. I have proof!\n\nAnyways! With that out of the way, let‚Äôs establish a few things:\n\nI have some experience with using Python for statistical work.\nMy initial introduction to computational statistics was in late 2021 (I had barely just turned 18! Time flies!).\nPython was not my introduction to coding or computer science as a whole (more on this in a minute).\nI have more experience with R (multiple semesters, multiple months) for this type of work than I do with Python (a few weeks at best like 3 years ago).\nI was being honest about finding R to be easier for me. What I didn‚Äôt know and somewhat bullshitted on the spot was why that was the case.\n\nThe entire point of this article (that will only be read by future me lol) is to revisit that last point and figure out why. I‚Äôm pretty curious about it. I pretty much just fumbled around to my teacher, saying, ‚ÄúI don‚Äôt know, it just feels easier to me,‚Äù in different ways with no actual substance. Well, let‚Äôs find that substance now!",
    "crumbs": [
      "Digital Garden",
      "Why R > Python IMO"
    ]
  },
  {
    "objectID": "garden/crab-cult/crab-cult.html#r-is-easier-than-python-huh",
    "href": "garden/crab-cult/crab-cult.html#r-is-easier-than-python-huh",
    "title": "Maybe I, Too, Am a Member of the Crustacean Cult",
    "section": "",
    "text": "I did oral exams disguised with a mustache and a hat mock job interviews in my Statistical Machine Learning class this past semester. I listed both Python and R (along with relevant libraries and packages) on my resume, and one question I had NOT considered before came up towards the start of my first interview.\nI‚Äôm paraphrasing here, but the gist is: ‚ÄúI see that you have both Python and R on your resume‚Ä¶ which do you prefer, and why?‚Äù\nI think I deserve a silver medal in bullshitting. I‚Äôm a modest person, so I can acknowledge that my performance wasn‚Äôt worthy of a gold medal.\nBut anyways, I was only half bullshitting with my answer. And even then, I wasn‚Äôt really bullshitting. I did answer to the best of my abilities. But man, did it feel like I was bullshitting my teacher (if you‚Äôre reading this, I‚Äôm sorry).\nI gave the only answer I could conceive of: ‚ÄúBecause R feels easier to me.‚Äù\nI walked right into a trap with that one. Notice I didn‚Äôt say better (which would have given me so much ammo for bullshitting), rather, I said easier. My teacher hit me back with the ‚Äúbut why?‚Äù And, I mean, I can‚Äôt blame him for that. He made valid points: Python is a beginner-friendly language, and most people consider it to be more straightforward and intuitive. Naturally, most students in my class would likely say scikit-learn &gt; tidymodels (I promise this is NOT an ‚ÄúI‚Äôm not like other programmers!‚Äù article).\nOh, did I mention that I haven‚Äôt written anything computational-statistics-wise with Python in years? My last (and first, and only) experience in this domain was with a Statistics for Data Science with Python MOOC I took on Coursera in‚Ä¶ 2022 (late 2021, really), along with some tinkering with scikit-learn I did shortly thereafter.\nNow, now, you just have to believe me on that one. I promise I‚Äôm not exercising my bullshitting skills again right now. I have proof!\n\nAnyways! With that out of the way, let‚Äôs establish a few things:\n\nI have some experience with using Python for statistical work.\nMy initial introduction to computational statistics was in late 2021 (I had barely just turned 18! Time flies!).\nPython was not my introduction to coding or computer science as a whole (more on this in a minute).\nI have more experience with R (multiple semesters, multiple months) for this type of work than I do with Python (a few weeks at best like 3 years ago).\nI was being honest about finding R to be easier for me. What I didn‚Äôt know and somewhat bullshitted on the spot was why that was the case.\n\nThe entire point of this article (that will only be read by future me lol) is to revisit that last point and figure out why. I‚Äôm pretty curious about it. I pretty much just fumbled around to my teacher, saying, ‚ÄúI don‚Äôt know, it just feels easier to me,‚Äù in different ways with no actual substance. Well, let‚Äôs find that substance now!",
    "crumbs": [
      "Digital Garden",
      "Why R > Python IMO"
    ]
  },
  {
    "objectID": "garden/crab-cult/crab-cult.html#my-programming-background",
    "href": "garden/crab-cult/crab-cult.html#my-programming-background",
    "title": "Maybe I, Too, Am a Member of the Crustacean Cult",
    "section": "My Programming Background",
    "text": "My Programming Background\nMy earliest serious experience with programming was when I was around 15 years old, when I learned‚Ä¶ Pascal. (I‚Äôll edit this in the future after I manage to find receipts from‚Ä¶ so many years ago.)\nSo, excluding HTML, Swift, and SQL, my experience went something like this:\n\nPascal, then\nC, then\nPython\n\nOne of these is so very obviously not like the others, right? Right????? Everyone, point your finger at the one language that isn‚Äôt as explicit and structured as the others.\nI used Pascal and C in high school. I didn‚Äôt touch Python until I did the MOOC I mentioned earlier, and didn‚Äôt really get into it until I started college.\nAnd man, did I struggle with Python when I just started. I had the programming fundamentals locked down to some extent (at least for the Introduction to Computer Science class), but going from something as explicit as C to something like Python was a bit challenging.\nMy teacher said that I was making my code needlessly complex and that I was using ‚ÄúC brain‚Äù.",
    "crumbs": [
      "Digital Garden",
      "Why R > Python IMO"
    ]
  },
  {
    "objectID": "garden/crab-cult/crab-cult.html#how-it-all-comes-together",
    "href": "garden/crab-cult/crab-cult.html#how-it-all-comes-together",
    "title": "Maybe I, Too, Am a Member of the Crustacean Cult",
    "section": "How It All Comes Together",
    "text": "How It All Comes Together\nI‚Äôve adapted somewhat to Python now, but I think the reason I find R to be easier is because of my ‚ÄúC brain‚Äù, that is, because I got my start using more structured and explicit languages, and R gives me that kind of space.\nHear me out: Yes, R is a high-level, dynamic, and interpreted language. Not too different from Python, I guess. But R encourages more structure and explicitness than Python does; the latter is designed to be more intuitive by using natural language and a lot of magic behind the scenes.\nI feel more confident using tidymodels than scikit-learn. Boil that down to familiarity in structure, I suppose.",
    "crumbs": [
      "Digital Garden",
      "Why R > Python IMO"
    ]
  },
  {
    "objectID": "garden/crab-cult/crab-cult.html#enter-the-crab-cult",
    "href": "garden/crab-cult/crab-cult.html#enter-the-crab-cult",
    "title": "Maybe I, Too, Am a Member of the Crustacean Cult",
    "section": "Enter the Crab Cult",
    "text": "Enter the Crab Cult\nNow, you might be wondering, ‚ÄúHow on Earth is the title relevant?‚Äù And that, dear future me reader, is a valid question.\nFor the last few months, I‚Äôve been fascinated by Rust. And I, unsurprisingly, now want to rewrite everything in Rust. I‚Äôll let the following image explain this for me:\n\n\n\n\n\nBUT WHY?????????\nBecause Rust has more in common with C than Python (in my uneducated opinion, which could very well be wrong). AND IT LOOKS FUN!!!!\nThis:\n{Rust}\nstruct AboutMe {\n    pronouns: String,\n    majors: Vec&lt;String&gt;,\n    specialization: String,\n    languages: Vec&lt;String&gt;,\n    hobbies: Vec&lt;String&gt;,\n}\n\nfn main() {\n    let ash = AboutMe {\n        pronouns: String::from(\"She/her\"),\n        majors: vec![\n            String::from(\"Computer Science-Mathematics\"),\n            String::from(\"Philosophy\")\n        ],\n        specialization: String::from(\"Data Science\"),\n        languages: vec![\n            String::from(\"Python\"),\n            String::from(\"Rust\"),\n            String::from(\"SQL\"),\n            String::from(\"R\"),\n            String::from(\"C\"),\n        ],\n        hobbies: vec![\n            String::from(\"Music Curation\"),\n            String::from(\"Video Games\"),\n            String::from(\"Reading\"),\n        ],\n    };\n}\nis longer, sure, but so much more explicit than this:\n{Python}\nclass AboutMe:\n  def __init__(self):\n    self.name = \"ash\"\n    self.pronouns = \"She/her\"\n    self.majors = [\"Computer Science-Mathematics\", \"Philosophy\"]\n    self.specialization = \"Data Science\"\n    self.languages = [\"Python\", \"Rust\", \"SQL\", \"R\", \"C\"]\n    self.hobbies = [\"Music Curation\", \"Video Games\", \"Reading\"]\nWill you catch me trying to use Rust or R with SQL any time soon? Of course not, I‚Äôm trying to decompress this summer, not give myself extra stress (which means I will likely do this at 3 am on a random Tuesday morning when I feel unhinged). I can admit that the Python + SQL combo is pretty good, though, in my experience.\nAlas, if you‚Äôre still wondering how crabs or cults relate to any of this, it‚Äôs because:\n\nSome say that Rust has a cult following;\nRust‚Äôs mascot is a crab (which I think is better than Python‚Äôs mascot, so can I give it bonus points for that?).\n\nAnyways, let‚Äôs close this ramble with a classic: the crab rave! Good times. I felt so giddy and nostalgic watching this for the first time in years üò≠.\n\n\nAlright, I‚Äôm signing off for real now. I have wise crabs to consult.\n\n\n\n\n\n¬∑ ¬∑ ‚îÄ ¬∑ñ•∏¬∑ ‚îÄ ¬∑ ¬∑",
    "crumbs": [
      "Digital Garden",
      "Why R > Python IMO"
    ]
  },
  {
    "objectID": "projects/eugenics-journal-graphs/aoe-graphs.html",
    "href": "projects/eugenics-journal-graphs/aoe-graphs.html",
    "title": "An Investigation of the Annals of Eugenics (1925-1953)",
    "section": "",
    "text": "Do a test run to make sure plans are feasible.\nFinish the spreadsheet!\nNormalize author names (e.g.¬†figure out how to handle ‚ÄúMary N. Karn‚Äù and ‚ÄúM. N. Karn‚Äù).\nRemove as many filler words as possible before creating word cloud.\nNote any particularly interesting titles.\nWrite thoughts, analysis, takeaways, reflection.",
    "crumbs": [
      "Projects",
      "Annals of Eugenics Graphs"
    ]
  },
  {
    "objectID": "projects/eugenics-journal-graphs/aoe-graphs.html#rationale",
    "href": "projects/eugenics-journal-graphs/aoe-graphs.html#rationale",
    "title": "An Investigation of the Annals of Eugenics (1925-1953)",
    "section": "2.1 Rationale",
    "text": "2.1 Rationale\nThis is part of a larger project about the history of Eugenics at my college (The College of Idaho), in my state (Idaho), and in academia as a whole. Exploring the Annals of Eugenics was a way to see the type of work that was being published during its tenure (at least before it was renamed to the Annals of Human Genetics in 1954).",
    "crumbs": [
      "Projects",
      "Annals of Eugenics Graphs"
    ]
  },
  {
    "objectID": "projects/eugenics-journal-graphs/aoe-graphs.html#about-the-data",
    "href": "projects/eugenics-journal-graphs/aoe-graphs.html#about-the-data",
    "title": "An Investigation of the Annals of Eugenics (1925-1953)",
    "section": "2.2 About the Data",
    "text": "2.2 About the Data\nThis dataset is self made! I made a spreadsheet keeping track of the titles and authors of articles across years, volumes, and issues.",
    "crumbs": [
      "Projects",
      "Annals of Eugenics Graphs"
    ]
  },
  {
    "objectID": "projects/eugenics-journal-graphs/aoe-graphs.html#limitations",
    "href": "projects/eugenics-journal-graphs/aoe-graphs.html#limitations",
    "title": "An Investigation of the Annals of Eugenics (1925-1953)",
    "section": "5.1 Limitations",
    "text": "5.1 Limitations\nMaking the spreadsheet took a minute, but that‚Äôs what happens when you make your own dataset. It probably would have been easier to scrape the data, but that felt like it would‚Äôve taken more time to set up and figure out, and inserting all of this information in a spreadsheet was a decent way to pass time.",
    "crumbs": [
      "Projects",
      "Annals of Eugenics Graphs"
    ]
  },
  {
    "objectID": "projects/eugenics-journal-graphs/aoe-graphs.html#reflection",
    "href": "projects/eugenics-journal-graphs/aoe-graphs.html#reflection",
    "title": "An Investigation of the Annals of Eugenics (1925-1953)",
    "section": "5.2 Reflection",
    "text": "5.2 Reflection\n¬Ø\\_(„ÉÑ)_/¬Ø TBD",
    "crumbs": [
      "Projects",
      "Annals of Eugenics Graphs"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html",
    "href": "projects/census-data-hackathon/ar_hackathon.html",
    "title": "Predicting US Census Data with XGBoost",
    "section": "",
    "text": "This project is my final assignment for MAT-427: Statistical Machine Learning, where we were tasked with using data from the U.S. Census Bureau to predict whether or not someone earns more than $50,000 annually. This project is a hackathon, meaning I had to make trade-offs between accuracy and interpretability. Since accuracy is arguably most important in a hackathon, it was prioritized over interpretability (most evident in the ‚ÄúFinal Thoughts‚Äù section).\n\n\nSince accuracy was my highest priority, I used gradient boosted trees (XGBoost in particular) for my model. XGBoost is widely used in competitive machine learning for its ability to efficiently model complex nonlinear relationships while handling missing data and feature interactions with minimal preprocessing.",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#rationale",
    "href": "projects/census-data-hackathon/ar_hackathon.html#rationale",
    "title": "Predicting US Census Data with XGBoost",
    "section": "",
    "text": "Since accuracy was my highest priority, I used gradient boosted trees (XGBoost in particular) for my model. XGBoost is widely used in competitive machine learning for its ability to efficiently model complex nonlinear relationships while handling missing data and feature interactions with minimal preprocessing.",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#preparing-data-for-visualization",
    "href": "projects/census-data-hackathon/ar_hackathon.html#preparing-data-for-visualization",
    "title": "Predicting US Census Data with XGBoost",
    "section": "3.1 Preparing Data for Visualization",
    "text": "3.1 Preparing Data for Visualization\nWe will use recipes and workflows to preprocess the data before modeling. Consequently, to not conflict with that, I made a copy of the training data for visualization purposes.\n\n\nDuplicating Data Frames (expand to view code)\ncensus_eda &lt;- census_train\n\n\nI made mild changes to our graphing data: I removed trailing whitespace, replaced ? values with Unknown, turned categorical variables into factors, and removed the inconsistent period at the end of some of the income values.\n\n\nModifying EDA Data (expand to view code)\ncensus_eda &lt;- census_eda |&gt;\n  mutate(across(where(is.character), ~ str_trim(.))) |&gt;\n  mutate(across(where(is.character), ~ ifelse(. == \"?\", \"Unknown\", .))) |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;\n  mutate(income = as.factor(str_trim(str_replace_all(as.character(income), \"\\\\.\", \"\"))))",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#finding-the-best-predictors",
    "href": "projects/census-data-hackathon/ar_hackathon.html#finding-the-best-predictors",
    "title": "Predicting US Census Data with XGBoost",
    "section": "3.2 Finding the ‚ÄúBest‚Äù Predictors",
    "text": "3.2 Finding the ‚ÄúBest‚Äù Predictors\nI used feature selection to narrow our list of predictors to the most ‚Äúmeaningful‚Äù ones. To achieve this, I used stepwise selection (both forward and backward) and the BIC (Bayesian Information Criterion) to take a model with all predictors and return one with only the most impactful predictors.\n\n\nCreating General Model & Stepwise Selection w/ BIC (expand to view code)\neda_model &lt;- glm(income ~ ., data = census_eda, family = binomial)\nn &lt;- nrow(census_eda) # get number of observations for BIC calculation\nmodel_BIC &lt;- stepAIC(eda_model, direction = \"both\", k = log(n), trace = FALSE)\n\n\nBelow, we can see the most impactful predictors:\n\n\nDisplaying Most Impactful Predictors (expand to view code)\nformula(model_BIC)\n\n\nincome ~ age + workclass + fnlwgt + education + marital.status + \n    occupation + relationship + sex + capital.gain + capital.loss + \n    hours.per.week",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#univariate-graph",
    "href": "projects/census-data-hackathon/ar_hackathon.html#univariate-graph",
    "title": "Predicting US Census Data with XGBoost",
    "section": "3.3 Univariate Graph",
    "text": "3.3 Univariate Graph\nLooking at the distribution of our response variable, we can see that most observations have an annual income of less than $50,000.\n\n\nIncome Bar Chart (expand to view code)\nincome_univariate &lt;- ggplot(census_eda, aes(x = income)) +\n        geom_bar(fill = \"#412d5e\", alpha = 0.9) +\n        labs(\n          title = \"Distribution of Income\",\n          x = \"Income\",\n          y = \"Number of Observations\") +\n        theme_minimal() +\n        theme(\n          plot.caption = element_text(hjust = 0, size = 10),\n          axis.text.x = element_text(angle = 45, hjust = 1)\n          )\n\nincome_univariate",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#bivariate-graphs",
    "href": "projects/census-data-hackathon/ar_hackathon.html#bivariate-graphs",
    "title": "Predicting US Census Data with XGBoost",
    "section": "3.4 Bivariate Graphs",
    "text": "3.4 Bivariate Graphs\nSix graphs might be a bit too much, but I found the following interesting: the income plotted against type of work, occupation, marital status, sex, age, and hours worked weekly. My eye-opening takeaways were that self-employed individuals with a registered incorporated business and people married to civillian spouses were more likely to make more than $50,000.\n\nWork Type & Occupation Proportional Bar Charts (expand to view code)\nworkclass_bivariate &lt;- ggplot(census_eda, aes(x = workclass, \n                                              fill = income)) + \n  geom_bar(position = \"fill\") + \n  labs(\n    title = \"Relationship Between Income & Work Type\",\n    x = \"Work Type\",\n    y = \"Proportion of Observations\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 16),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\noccupation_bivariate &lt;- ggplot(census_eda, aes(x = occupation, \n                                               fill = income)) + \n  geom_bar(position = \"fill\") + \n  labs(\n    title = \"Relationship Between Income & Occupation\",\n    x = \"Occupation\",\n    y = \"Proportion of Observations\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 16),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\nworkclass_bivariate\noccupation_bivariate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarital Status & Sex Proportional Bar Charts (expand to view code)\nmarriage_bivariate &lt;- ggplot(census_eda, aes(x = marital.status, \n                                              fill = income)) + \n  geom_bar(position = \"fill\") + \n  labs(\n    title = \"Relationship Between Income & Marital Status\",\n    x = \"Marital Status\",\n    y = \"Proportion of Observations\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 16),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\nsex_bivariate &lt;- ggplot(census_eda, aes(x = sex, \n                                               fill = income)) + \n  geom_bar(position = \"fill\") + \n  labs(\n    title = \"Relationship Between Income & Sex\",\n    x = \"Sex\",\n    y = \"Proportion of Observations\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 16),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\nmarriage_bivariate\nsex_bivariate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge & Hours Worked Weekly Proportional Box Plots (expand to view code)\nage_bivariate &lt;- ggplot(census_eda, aes(x = income, y = age, fill = income)) +\n  geom_boxplot() +\n  labs(title = \"Relationship Between Income & Age\",\n       x = \"Income\",\n       y = \"Age\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\nhours_bivariate &lt;- ggplot(census_eda, aes(x = income, y = hours.per.week, fill = income)) +\n  geom_boxplot() +\n  labs(title = \"Relationship Between Income & Hours worked Weekly\",\n       x = \"Income\",\n       y = \"Hours Worked Weekly\") +\n  theme_minimal() +\n  scale_fill_wa_d(\"puget\") +\n  theme(\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14))\n\nage_bivariate\nhours_bivariate",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#preparing-data-for-modeling",
    "href": "projects/census-data-hackathon/ar_hackathon.html#preparing-data-for-modeling",
    "title": "Predicting US Census Data with XGBoost",
    "section": "4.1 Preparing Data for Modeling",
    "text": "4.1 Preparing Data for Modeling\nI created an XGBoost model with the goal of tuning its hyperparameters for better performance.\n\nboosted_model &lt;- boost_tree(trees = tune(), \n                            learn_rate = tune(), \n                            tree_depth = tune(), \n                            min_n = tune()) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\nI treated ? values as NA so they could be imputed. I also converted categorical variables into factors, ensured numeric predictors were integers, used the median to impute numeric values, and the mode for categorical ones. Finally, I applied dummy (one-hot) encoding so categorical variables could be modeled numerically.\n\nboosted_recipe &lt;- recipe(income ~ ., data = census_train) |&gt;\n  step_mutate(across(where(is.character) | where(is.factor), ~ as.factor(stringr::str_trim(ifelse(as.character(.x) == \"?\", NA, as.character(.x)))))) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_integer(all_numeric_predictors()) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nI made a workflow to put our recipe and model together so they could easily be used with the training (and later the test) set.\n\nboosted_workflow &lt;- workflow() |&gt;\n  add_model(boosted_model) |&gt;\n  add_recipe(boosted_recipe)\n\nI defined a grid of hyperparameter values for my XGBoost model using grid_space_filling(), which selects a diverse but efficient sample of combinations. I then applied 5-fold cross-validation (with one repeat) to evaluate each combination, stratifying by income to preserve class distribution across folds.\n\nboosted_grid &lt;- grid_space_filling(trees(range = c(20, 200)),\n                                   tree_depth(range = c(1, 15)),\n                                   learn_rate(range = c(-10,-1)),\n                                   min_n(range = c(2, 40)),\n                                   size = 50)\n\ncensus_folds = vfold_cv(census_train, v = 5, repeats = 1, strata = income)\n\nI used parallel processing to speed up tuning across my grid of hyperparameters. Using tune_grid(), I evaluated each model combination defined earlier using 5-fold cross-validation. The process was parallelized across 9 cores to reduce runtime, and the cluster was stopped afterward to free system resources.\n\ncl &lt;- makePSOCKcluster(9)\nregisterDoParallel(cl)\n\ntuning_results &lt;- tune_grid(\n  boosted_workflow,\n  resamples = census_folds,\n  grid = boosted_grid\n)\n\nstopCluster(cl)\n\nAfter tuning, I selected the hyperparameter combination that gave the highest accuracy and used it to train the final model.\n\n\nFitting Model w/ Tuned Parameters (expand to view code)\nboosted_fit &lt;- boosted_workflow |&gt;\n  finalize_workflow(select_best(tuning_results, metric = \"accuracy\")) |&gt;\n  fit(census_train)\n\n\nFinally, I used the tuned model to generate predictions on the test set. Since the test set didn‚Äôt include income labels, I converted the predicted class into a binary format (1 for ‚Äú&gt;50K‚Äù, 0 otherwise) and exported the results as a .csv file.\n\nprediction_vector &lt;- predict(boosted_fit, new_data = census_test, type = \"class\") |&gt;\n  mutate(income = ifelse(.pred_class %in% c(\"&gt;50K\", \"&gt;50K.\"), 1, 0)) |&gt;\n  select(income)\n\nwrite.csv(prediction_vector, \"my_predictions.csv\", row.names = FALSE)",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#update",
    "href": "projects/census-data-hackathon/ar_hackathon.html#update",
    "title": "Predicting US Census Data with XGBoost",
    "section": "5.1 Update",
    "text": "5.1 Update\nI came third (out of 15 people)!!! My accuracy was 86.6%. I will be returning to this in the near future because I am curious about pairing my stepwise BIC model WITH XGBoost and seeing what those results would be.",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/census-data-hackathon/ar_hackathon.html#limitations",
    "href": "projects/census-data-hackathon/ar_hackathon.html#limitations",
    "title": "Predicting US Census Data with XGBoost",
    "section": "5.2 Limitations",
    "text": "5.2 Limitations\nI don‚Äôt really have anything to report on my final results. One reason is that while I had the test data, it actually does not have the income column. I will likely make an update here (once my teacher has evaluated my work) with my accuracy, and if possible, a confusion matrix and other metrics.\nGradient boosted trees are computationally expensive, meaning every mistake and consequent re-run cost me over 2 additional hours of runtime. That held me back from experimenting more, to be honest. I only repeated my cross-validation once as I suspect it would take 20+ hours if I did it 10 times (running my current setup with 1 fold took &gt; 2 hours).",
    "crumbs": [
      "Projects",
      "Census Data Hackathon"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "",
    "text": "This project investigates how various character traits influence spending behaviors in adults (18+). Using data from the National Financial Well-Being Survey (collected by the Consumer Financial Protection Bureau), I explored how these factors impact the likelihood of exhibiting ‚Äúgood‚Äù or ‚Äúbad‚Äù spending habits across different age groups.\n\n\nI found this an interesting topic as it prompted me to reflect on my own spending habits.\n\n\n\nThe data was collected from a diverse sample of adults (18 and older) across all 50 U.S. states and the District of Columbia, conducted between October 27 and December 5, 2016. The sample included 6,394 individuals (5,295 from the general population and 999 from an oversample of adults aged 62 and over). This dataset is randomized and aims to be representative of the adult population.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#rationale",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#rationale",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "",
    "text": "I found this an interesting topic as it prompted me to reflect on my own spending habits.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#about-the-data",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#about-the-data",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "",
    "text": "The data was collected from a diverse sample of adults (18 and older) across all 50 U.S. states and the District of Columbia, conducted between October 27 and December 5, 2016. The sample included 6,394 individuals (5,295 from the general population and 999 from an oversample of adults aged 62 and over). This dataset is randomized and aims to be representative of the adult population.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#counting-refused-questions",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#counting-refused-questions",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "3.1 Counting Refused Questions",
    "text": "3.1 Counting Refused Questions\nEach column, except for wellbeing, represents a question answered by a respondent (wellbeing is a score given to the respondent based on answers to particular questions). ‚Äú-1‚Äù means that the respondent refused to answer the question. I opted to count the number of questions (out of a total of 16) unanswered by each respondent and calculated a ‚Äúrefusal rate‚Äù for enhanced readability (‚Äúthis person refused to answer 75% of the included questions‚Äù sounds nicer than ‚Äúthis person refused to answer 12 of the included questions‚Äù).\nThis led to the creation of two additional columns: refused_questions for the raw number and refusal_rate for the percentage.\n\n\nCounting Refusals (expand to view code)\nfinance_analysis &lt;- finance_analysis |&gt;\n  mutate(\n    refused_questions = rowSums(across(-c(wellbeing), ~ .x == -1))\n    )\n\n\n\n\nCalculating the Refusal Rate (expand to view code)\n#I counted the number of (relevant) columns to avoid hard coding \nnum_columns &lt;- ncol(finance_analysis)\nexcluded_columns &lt;- list(\"wellbeing\", \"refused_questions\")\nnum_excluded &lt;- length(excluded_columns)\nsum_questions &lt;- num_columns - num_excluded\n\nfinance_analysis &lt;- finance_analysis |&gt;\n  mutate(\n    refusal_rate = round((refused_questions / sum_questions) * 100,\n                         2)\n  )",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#encoding-missing-responses",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#encoding-missing-responses",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "3.2 Encoding Missing Responses",
    "text": "3.2 Encoding Missing Responses\nEvery column except for age_group, refused_questions, and refusal_rate contained at least one -1 value.\n\n\nIdentifying Columns with Refusals (expand to view code)\ncontains_refusals &lt;- finance_analysis |&gt;\n  summarise(across(everything(), ~ any(. &lt; 0, na.rm = TRUE))) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"has_refusal\")\n\ncontains_refusals |&gt; kable()\n\n\n\n\n\nvariable\nhas_refusal\n\n\n\n\nfollow_commitment\nTRUE\n\n\nfrugality\nTRUE\n\n\nworded_probability\nTRUE\n\n\npercentage_skill\nTRUE\n\n\ngoal_confidence\nTRUE\n\n\nadmire_luxury\nTRUE\n\n\nself_worth\nTRUE\n\n\nimpress_people\nTRUE\n\n\npsych\nTRUE\n\n\ndistress\nTRUE\n\n\nimpulsivity\nTRUE\n\n\nresist_temptation\nTRUE\n\n\nlong_term_goals\nTRUE\n\n\neconomic_mobility\nTRUE\n\n\nspending_habit\nTRUE\n\n\nage_group\nFALSE\n\n\nwellbeing\nTRUE\n\n\nrefused_questions\nFALSE\n\n\nrefusal_rate\nFALSE\n\n\n\n\n\nI handled this in multiple ways:\n\nI replaced -1 with ‚Äúrefused‚Äù for the categorical variables, treating it as a separate category for data visualization purposes.\n\n\nRenaming Refusals (expand to view code)\nfinance_analysis &lt;- finance_analysis |&gt;\n  mutate(\n    across(-c(psych, wellbeing), ~ ifelse(. == -1, \"refused\", .))\n    )\n\n\n\n\n\nSince psych and wellbeing are quantitative variables, I replaced -1 values with numbers 10% below their minimums to preserve visual consistency on graphs. Note that wellbeing also contained a -4 value, meaning ‚Äúresponse not written to database‚Äù. Since there was only one instance of this, I used a value 30% lower than the minimum to replace it.\n\n\nCalculating Minimum and Replacement Values (expand to view code)\npsych_min &lt;- min(finance_analysis$psych)\npsych_refuse &lt;- psych_min - ((10/100) * psych_min)\n\nwellbeing_min &lt;- min(finance_analysis$wellbeing)\nwellbeing_refuse &lt;- wellbeing_min - ((10/100) * wellbeing_min)\nwellbeing_unwritten &lt;- wellbeing_min - ((30/100) * wellbeing_min)\n\n\n\n\nRe-encoding Refusals for Quantitative Variables (expand to view code)\nfinance_analysis &lt;- finance_analysis |&gt;\n  mutate(\n    psych = if_else(psych == -1, psych_refuse, psych),\n    wellbeing = case_when(\n      wellbeing == -4 ~ wellbeing_unwritten,\n      wellbeing == -1 ~ wellbeing_refuse,\n      TRUE ~ wellbeing)\n    )",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#wrapping-up",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#wrapping-up",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "3.3 Wrapping Up",
    "text": "3.3 Wrapping Up\nTo wrap things up, I factored the categorical variables so that responses would be grouped accordingly on graphs.\n\n\nFactoring Categorical Variables (expand to view code)\nfinance_analysis &lt;- finance_analysis |&gt;\n  mutate(\n    spending_habit = factor(spending_habit),\n    follow_commitment = factor(follow_commitment),\n    frugality = factor(frugality),\n    worded_probability = factor(worded_probability),\n    percentage_skill = factor(percentage_skill),\n    goal_confidence = factor(goal_confidence),\n    admire_luxury = factor(admire_luxury),\n    self_worth = factor(self_worth),\n    impress_people = factor(impress_people),\n    distress = factor(distress),\n    impulsivity = factor(impulsivity),\n    resist_temptation = factor(resist_temptation),\n    long_term_goals = factor(long_term_goals),\n    economic_mobility = factor(economic_mobility),\n    age_group = factor(age_group)\n  )\n\n\nLastly, I saved the modified data frame as an .rds file to use it in the Shiny apps I created for visualization.\n\n\nSaving Data Frame (expand to view code)\nsaveRDS(finance_analysis, file = \"shiny-apps/data/finance_analysis.rds\")",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#visual-overview-response-variable",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#visual-overview-response-variable",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "4.1 Visual Overview: Response Variable",
    "text": "4.1 Visual Overview: Response Variable\nMost responses in our outcome variable fell in group 4, meaning respondents generally answered ‚Äúvery well‚Äù to the prompt ‚ÄúI know how to keep myself from spending too much.‚Äù Group 1 (‚Äúnot at all‚Äù) had the fewest responses.\nIf we were to reimagine this variable as binary, i.e., grouping responses 1‚Äì3 as ‚Äúlower spending control‚Äù and responses 4‚Äì5 as ‚Äúhigher spending control‚Äù, the majority of the responses would fall into the latter category.\n\n\nSpending Habit Bar Chart (expand to view code)\nspending_univariate &lt;- ggplot(finance_analysis, aes(x = spending_habit)) +\n        geom_bar(fill = \"#412d5e\", alpha = 0.9) +\n        labs(\n          title = \"Distribution of Spending Habit Scores\",\n          x = \"Scores (Ranging From '1: Not At All' to '5: Completely')\",\n          y = \"Number of Respondents\",\n          caption = \"Prompt: 'I know how to keep myself from spending too much.'\") +\n        theme_minimal() +\n        theme(\n          plot.caption = element_text(hjust = 0, size = 10),\n          axis.text.x = element_text(angle = 45, hjust = 1)\n          )\n\nspending_univariate",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#bivariate-patterns",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#bivariate-patterns",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "4.2 Bivariate Patterns",
    "text": "4.2 Bivariate Patterns\nBoth frugality and follow_commitment show a positive trend: as spending_habit scores increase, the corresponding predictor scores also increase.\nThis relationship is visually evident in the increasing density of points in the upper right areas of both plots, suggesting that respondents with higher reported spending habit scores also report higher frugality and stronger follow-through on commitments.\n\nSpending Habit & Frugality/Following Commitments Jitter Plots (expand to view code)\nfrugality_bivariate &lt;- ggplot(finance_analysis, aes(x = frugality, \n                                                    y = spending_habit, \n                                                    color = spending_habit)) + \n  geom_jitter() + \n  labs(\n    title = \"Relationship between spending_habit & frugality\",\n    x = \"Frugality Score\",\n    y = \"Spending Habit Score\",\n    subtitle = \"'I know how to keep myself from spending too much' & 'If I can reuse an item I already have, there's \\nno sense in buying something new'\",\n    caption = \"* 'Spending Habits' scores range from 'not at all' (1) to 'completely' (5) \\n* 'Frugality' scores range from 'strongly disagree' (1) to 'strongly agree' (6)\") +\n  theme_minimal() +\n  scale_color_wa_d(\"stuart\") +\n  theme(\n    plot.caption = element_text(hjust = 0, size = 10),\n    plot.subtitle = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n\ncommitment_bivariate &lt;- ggplot(finance_analysis, aes(x = follow_commitment, \n                                             y = spending_habit, \n                                             color = spending_habit)) + \n  geom_jitter() + \n  labs(\n    title = \"Relationship between spending_habit & follow_commitment\",\n    x = \"Following Commitment Score\",\n    y = \"Spending Habit Score\",\n    subtitle = \"'I know how to keep myself from spending too much' & 'I follow-through on my financial \\ncommitments to others'\",\n    caption = \"* 'Spending Habits' scores range from 'not at all' (1) to 'completely' (5) \\n* 'Follow Commitment' scores range from 'not at all' (1) to 'completely' (5)\") +\n  theme_minimal() +\n  scale_color_wa_d(\"stuart\") +\n  theme(\n    plot.caption = element_text(hjust = 0, size = 10),\n    plot.subtitle = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n\nfrugality_bivariate\ncommitment_bivariate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth impress_people and impulsivity display the opposite pattern: a negative relationship with spending_habit. As spending_habit scores increase, these predictor scores decrease, which can be seen in the decreasing density of points across the x-axis.\nThis suggests that respondents with higher reported spending habit scores tend to report lower impulsivity and desire to impress others.\n\nSpending Habit & Impressing People/Impulsivity Jitter Plots (expand to view code)\nimpress_bivariate &lt;- ggplot(finance_analysis, aes(x = impress_people, \n                                                    y = spending_habit, \n                                                    color = spending_habit)) + \n  geom_jitter() + \n  labs(\n    title = \"Relationship between spending_habit & impress_people\",\n    x = \"Impressing People Score\",\n    y = \"Spending Habit Score\",\n    subtitle = \"'I know how to keep myself from spending too much' & 'I like to own things that impress people'\",\n    caption = \"* 'Spending Habits' scores range from 'not at all' (1) to 'completely' (5) \\n* 'Impress People' scores range from 'strongly disagree' (1) to 'strongly agree' (5)\") +\n  theme_minimal() +\n  scale_color_wa_d(\"stuart\") +\n  theme(\n    plot.caption = element_text(hjust = 0, size = 10),\n    plot.subtitle = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n\nimpulsivity_bivariate &lt;- ggplot(finance_analysis, aes(x = impulsivity, \n                                             y = spending_habit, \n                                             color = spending_habit)) + \n  geom_jitter() + \n  labs(\n    title = \"Relationship between spending_habit & impulsivity\",\n    x = \"Impulsivity Score\",\n    y = \"Spending Habit Score\",\n    subtitle = \"'I know how to keep myself from spending too much' & 'I often act without thinking \\nthrough all the alternatives'\",\n    caption = \"* 'Spending Habits' scores range from 'not at all' (1) to 'completely' (5) \\n* 'Impulsivity' scores range from 'not at all' (1) to 'completely well' (4)\") +\n  theme_minimal() +\n  scale_color_wa_d(\"stuart\") +\n  theme(\n    plot.caption = element_text(hjust = 0, size = 10),\n    plot.subtitle = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n\nimpress_bivariate\nimpulsivity_bivariate",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#data-dictionary-univariate-plots",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#data-dictionary-univariate-plots",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "5.1 Data Dictionary & Univariate Plots",
    "text": "5.1 Data Dictionary & Univariate Plots\nThis interactive app includes a data dictionary and visual summaries of how responses are distributed across each variable.\n\n\n5.1.1 Calculating Highlighted Points for Density Plots\nIn two of the distribution graphs (the density plots for the quantitative predictors wellbeing and psych), specific points are highlighted to represent refused or unrecorded responses. These were plotted using density() to estimate the distribution of the variable and approx() to get the approximate y-value at the specified points.\n\n\nCalculating Highlighted Points (expand to view code)\npsych_dens &lt;- density(finance_analysis$psych, na.rm = TRUE)\npsych_r_dens &lt;- approx(psych_dens$x, psych_dens$y, xout = psych_refuse)$y\n\nwellbeing_dens &lt;- density(finance_analysis$wellbeing, na.rm = TRUE)\nwellbeing_r_dens &lt;- approx(wellbeing_dens$x, wellbeing_dens$y, xout = wellbeing_refuse)$y\nwellbeing_m_dens &lt;- approx(wellbeing_dens$x, wellbeing_dens$y, xout = wellbeing_unwritten)$y",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#bivariate-plots",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#bivariate-plots",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "5.2 Bivariate Plots",
    "text": "5.2 Bivariate Plots\nThis interactive app visually explores the relationship between each predictor and the response variable.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#preparing-data-for-modeling",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#preparing-data-for-modeling",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "6.1 Preparing Data for Modeling",
    "text": "6.1 Preparing Data for Modeling\nAs was done before, I created a new column to hold the number of refused questions per observation.\n\n\nCounting Refusals (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n    refused_questions = rowSums(across(-c(wellbeing), ~ .x == -1))\n    )\n\n\nTo prepare the missing (refused/unrecorded) values for imputation, I re-encoded them as NA.\n\n\nRe-encoding Missing Values as NA (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n    across(-all_of(\"refused_questions\"), ~ ifelse(. %in% c(-1, -4), NA, .))\n  )\n\n\nI used the median to impute missing values in the categorical variables, since, despite being categorical, they were also ordinal (i.e., there was a meaningful order between categories).\n\n\nImputing Missing Values in Qualitative Variables (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n      across(-c(\"psych\", \"wellbeing\", \"refused_questions\", \"age_group\"), impute_median)\n      )\n\n\nI used histograms to determine how to approach imputation for the quantitative variables. Since psych was negatively skewed and wellbeing was approximately normally distributed, I imputed missing values using the median for psych and the mean for wellbeing.\n\nExploring Distribution of Quantitative Variables (expand to view code)\ngf_histogram(~psych, data = finance_modeling, fill = \"#412d5e\", alpha = 0.9) +\n  theme_minimal()\ngf_histogram(~wellbeing, data = finance_modeling, fill = \"#412d5e\", alpha = 0.9) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputing Missing Values in Quantitative Variables (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n      psych = impute_median(psych)\n      )\n\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n      wellbeing = impute_mean(wellbeing)\n      )\n\n\nSince this is a binary logistic regression, I created a new variable representing a binary version of the original response.\nCategory 1 includes responses 4 and 5, indicating excellent reported spending habits, while Category 0 includes responses 1 through 3, indicating comparatively weaker reported spending habits.\n\n\nCreating Binary Response Variable (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n     spending_binary = if_else(spending_habit &gt;= 4, 1, 0)\n  )\n\n\nTo tie it all together, I factored the categorical variables (again).\n\n\nFactoring Categorical Variables (expand to view code)\nfinance_modeling &lt;- finance_modeling |&gt;\n  mutate(\n    spending_habit = factor(spending_habit),\n    follow_commitment = factor(follow_commitment),\n    frugality = factor(frugality),\n    worded_probability = factor(worded_probability),\n    percentage_skill = factor(percentage_skill),\n    goal_confidence = factor(goal_confidence),\n    admire_luxury = factor(admire_luxury),\n    self_worth = factor(self_worth),\n    impress_people = factor(impress_people),\n    distress = factor(distress),\n    impulsivity = factor(impulsivity),\n    resist_temptation = factor(resist_temptation),\n    long_term_goals = factor(long_term_goals),\n    economic_mobility = factor(economic_mobility),\n    age_group = factor(age_group)\n  )",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#models-metrics-thoughts",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#models-metrics-thoughts",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "6.2 Models, Metrics, & Thoughts",
    "text": "6.2 Models, Metrics, & Thoughts\nFirst, I fitted a binary logistic regression model and included all of the predictor variables. This served as a baseline for feature selection.\n\n\nFitting General Model With All Predictors (expand to view code)\ngeneral_model &lt;- glm(spending_binary ~ . - spending_habit, data = finance_modeling, family = \"binomial\")\n\n\n\n6.2.1 Feature Selection\nI used stepwise selection (both forward and backward) to identify the ‚Äúbest‚Äù set of predictors (i.e., predictors with a statistically meaningful relationship to the response variable). This was done twice: once using AIC (Akaike Information Criterion), and once using BIC (Bayesian Information Criterion).\nAIC and BIC are model selection metrics, but they have different priorities. AIC prioritizes predictive performance by balancing model fit and model complexity, while BIC prioritizes simpler models by applying a stronger penalty for model complexity.\nIn other words, AIC generally favors more complex models that include predictors contributing meaningfully to accuracy, whereas BIC is stricter and prefers a simpler model with only the most impactful predictors.\nApplied here, AIC is used to identify predictors that have some relationship with the response, while BIC narrows our focus to the strongest predictors.\n\n\nStepwise Selection with AIC (expand to view code)\nmodel_AIC &lt;- stepAIC(general_model, direction = \"both\", k = 2, trace = FALSE)\n\n\n\n\nStepwise Selection with BIC (expand to view code)\n# I needed to get the number of observations\nn &lt;- nrow(finance_modeling)\n\nmodel_BIC &lt;- stepAIC(general_model, direction = \"both\", k = log(n), trace = FALSE)\n\n\n\n6.2.1.1 Examining the AIC Model\nAccording to the AIC model, the most indicative predictors of spending habits include:\n\nFinancial behaviors: ability to follow [financial] commitments, be frugal, resist temptation, and work toward long-term goals; financial well-being score\nCognitive preferences: preference for numbers over words and comfort with percentages\nPsychological factors: confidence in financial goals, desire to impress others, psychological connectedness, stress, and impulsivity\nDemographics: age group\n\n\n\nAIC Model Formula (expand to view code)\nformula(model_AIC)\n\n\nspending_binary ~ follow_commitment + frugality + worded_probability + \n    percentage_skill + goal_confidence + impress_people + psych + \n    distress + impulsivity + resist_temptation + long_term_goals + \n    age_group + wellbeing\n\n\n\n\n6.2.1.2 Examining the BIC Model\nAccording to the BIC model, the strongest predictors related to one‚Äôs spending habits are\n\nAbility to follow commitments\nFrugality\nConfidence in achieving financial goals\nImpulsivity\nAbility to resist temptation\nFinancial well-being score\n\n\n\nBIC Model Formula (expand to view code)\nformula(model_BIC)\n\n\nspending_binary ~ follow_commitment + frugality + goal_confidence + \n    impulsivity + resist_temptation + wellbeing\n\n\n\n\n\n6.2.2 Model Assumptions\nSuccessful logistic regression requires a few assumptions, namely:\n\nLinearity of Log-Odds\nI assessed this by plotting the log-odds against the predictors. This required a few steps:\n\n\nMaking a New Data Frame With Predictors and Response From Model (expand to view code)\nbic_finance &lt;- finance_modeling |&gt;\n  dplyr::select(spending_binary, follow_commitment, frugality, goal_confidence, impulsivity, resist_temptation, wellbeing)\n\n\n\n\nExtracting Predicted Probabilities (expand to view code)\npredictors &lt;- colnames(bic_finance) \n\nbic_finance$probabilities &lt;- model_BIC$fitted.values\n\n\n\n\nCalculating Logit Values (expand to view code)\nbic_finance &lt;- bic_finance |&gt;\n  mutate(logit = log(probabilities/(1-probabilities))) |&gt;\n  dplyr::select(-probabilities) |&gt;\n  gather(key = \"predictors\", value = \"predictor.value\", -logit) \n\n\n\n\nDisplaying Log-Odds Against Predictors (expand to view code)\nlog_plot &lt;- ggplot(bic_finance, aes(y = logit, x = predictor.value))+\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"loess\") + \n  theme_bw() + \n  facet_wrap(~predictors, scales = \"free_x\")\n\nlog_plot\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe categorical (ordinal) predictors appeared stable across levels. The continuous variable wellbeing showed a roughly linear trend, suggesting the assumption of linearity on the logit scale was reasonably met.\nTo avoid redundancy, I only did this for the BIC model. We can reasonably assume that the assumption holds true for the AIC model.\nIndependence of Observations\nEach observation represents a unique, individual survey response. No groups (such as same households or repeat responses) are present. Therefore, the model meets this assumption.\nAbsence of Multicollinearity\nI used vif() to look for potential multicollinearity.\nAIC Model:\n\n\nAIC Model Variance (expand to view code)\nvif_aic &lt;- vif(model_AIC)\nvif_aic |&gt; kable()\n\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nfollow_commitment\n1.325389\n4\n1.035841\n\n\nfrugality\n1.273549\n5\n1.024475\n\n\nworded_probability\n1.293823\n5\n1.026095\n\n\npercentage_skill\n1.396970\n5\n1.033996\n\n\ngoal_confidence\n1.664631\n3\n1.088645\n\n\nimpress_people\n1.299022\n4\n1.033242\n\n\npsych\n1.160909\n1\n1.077455\n\n\ndistress\n1.423641\n4\n1.045141\n\n\nimpulsivity\n1.294653\n3\n1.043980\n\n\nresist_temptation\n1.455748\n3\n1.064587\n\n\nlong_term_goals\n1.651653\n3\n1.087226\n\n\nage_group\n1.388850\n7\n1.023740\n\n\nwellbeing\n1.676884\n1\n1.294945\n\n\n\n\n\nThe \\(GVIF^{(\\frac{1}{2\\times Df})}\\) column contains adjusted VIF values. All values are below 2, indicating no serious multicollinearity.\nBIC Model:\n\n\nBIC Model Variance (expand to view code)\nvif_bic &lt;- vif(model_BIC)\nvif_bic |&gt; kable()\n\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nfollow_commitment\n1.214668\n4\n1.024607\n\n\nfrugality\n1.131563\n5\n1.012437\n\n\ngoal_confidence\n1.421325\n3\n1.060349\n\n\nimpulsivity\n1.146548\n3\n1.023054\n\n\nresist_temptation\n1.163430\n3\n1.025550\n\n\nwellbeing\n1.354680\n1\n1.163907\n\n\n\n\n\nSimilarly, all values in the \\(GVIF^{(\\frac{1}{2\\times Df})}\\) column are less than 2, once again indicating that there is no notable multicollinearity.\nNo (Influential) Outliers I used standardized residuals to identify potential outliers. No observations exceeded the common threshold of ¬±3, suggesting the model fits individual data points well and meets the assumption of no extreme residuals.\n\n\nAugmenting Model and Filtering Residuals (expand to view code)\nmodel_info &lt;- augment(model_BIC)\n\nmodel_info |&gt;\n  filter(abs(.std.resid) &gt; 3)\n\n\n# A tibble: 0 √ó 13\n# ‚Ñπ 13 variables: spending_binary &lt;dbl&gt;, follow_commitment &lt;fct&gt;,\n#   frugality &lt;fct&gt;, goal_confidence &lt;fct&gt;, impulsivity &lt;fct&gt;,\n#   resist_temptation &lt;fct&gt;, wellbeing &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nBinary/Ordinal Dependent Variable\n\nI used spending_binary as the response variable. Consequently, both models meet this assumption.\n\n\n\nTo wrap things up, I ranked the predictors based on their coefficients. The higher the coefficient is, the stronger the relationship.\n\n\nRanking Predictors Based On Effect (expand to view code)\ncoef_summary &lt;- summary(model_BIC)$coefficients\n\n# I had to turn the summary into a data frame so I could group by variable\ncoef_df &lt;- as.data.frame(coef_summary) |&gt;\n  mutate(term = rownames(coef_summary)) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(variable = str_remove(term, \"\\\\d+$\")) |&gt;\n  group_by(variable) |&gt;\n  summarise(total_effect = sum(abs(Estimate))) |&gt;\n  arrange(desc(total_effect))\n\ncoef_df |&gt; kable()\n\n\n\n\n\nvariable\ntotal_effect\n\n\n\n\nfrugality\n3.7410909\n\n\nresist_temptation\n2.9465421\n\n\nfollow_commitment\n1.7087117\n\n\nimpulsivity\n1.3665486\n\n\ngoal_confidence\n1.2320587\n\n\nwellbeing\n0.0358059\n\n\n\n\n\nOur takeaway here is that frugality and resist_temptation are the strongest factors contributing to a respondent‚Äôs spending_habit score.\nI used a predicted probabilities plot to visualize how these two factors affected our model.\n\n\nPredicted Probabilities Plot (frugality & resisting temptation) (expand to view code)\nget_mode &lt;- function(x) {\n  ux &lt;- na.omit(x)\n  ux[which.max(tabulate(match(ux, ux)))]\n}\n\ngrid &lt;- expand.grid(\n  impulsivity = get_mode(finance_modeling$impulsivity),\n  resist_temptation = sort(unique(finance_modeling$resist_temptation)),\n  follow_commitment = get_mode(finance_modeling$follow_commitment),\n  frugality = sort(unique(finance_modeling$frugality)),\n  goal_confidence = get_mode(finance_modeling$goal_confidence),\n  wellbeing = mean(finance_modeling$wellbeing, na.rm = TRUE)\n)\n\ngrid$predicted_prob &lt;- predict(model_BIC, newdata = grid, type = \"response\")\n\npredicted_probability_plot &lt;- ggplot(grid, aes(x = frugality, y = predicted_prob, color = resist_temptation, group = resist_temptation)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  scale_color_wa_d(\"stuart\") +\n  theme(plot.subtitle = element_text(size = 9),\n        plot.caption = element_text(hjust = 0))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nPredicted Probabilities Plot (frugality & resisting temptation) (expand to view code)\npredicted_probability_plot\n\n\n\n\n\n\n\n\n\nAs frugality and resist_temptation scores increased, so did the predicted probability of having a ‚Äúgood‚Äù spending_habit score.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "projects/spending-behavior-model/spending-habits-analysis.html#limitations",
    "href": "projects/spending-behavior-model/spending-habits-analysis.html#limitations",
    "title": "Analyzing Spending Habits With Logistic Regression",
    "section": "7.1 Limitations",
    "text": "7.1 Limitations\nScores were self-reported, and, given the nature of the topic, likely unreliable.",
    "crumbs": [
      "Projects",
      "Spending Habits Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Ash‚Äôs portfolio! :)",
    "section": "",
    "text": "‚ô° Click here to view my projects! ‚ô°\nHi, I‚Äôm Ash. I‚Äôm a junior double majoring in Philosophy and Mathematics-Computer Science with a specialization in Data Science. I‚Äôm especially interested in the philosophy of mind and the ways data science and philosophy intersect, particularly when it comes to understanding human behavior and decision-making. Data science lets me explore real-world problems, while my background in philosophy helps me think critically about the implications of what I find.\nWhen I‚Äôm not working on school stuff, you can find me playing indie games like Stardew Valley or Balatro. I‚Äôm a fan of all sorts of music, from jazz to hyperpop to rock. I also love cozy things; hot chocolate, blankets, and autumn are some of my favorite comforts. If I‚Äôm not playing video games or listening to music, I‚Äôll probably be baking cupcakes (they‚Äôre easier to share) for my friends or reading articles from The New York Times (I have a soft spot for Modern Love especially), The Atlantic, or WIRED.\n\nYou can find my GitHub here.\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "garden/garden-home.html",
    "href": "garden/garden-home.html",
    "title": "Ash‚Äôs Digital Garden",
    "section": "",
    "text": "Collection of thoughts and long-form written pieces. Usually about programming, artificial intelligence, and/or philosophy.\nTo be honest, this collection exists so I can become more comfortable with expressing my opinions. My hope is to make my philosophy senior thesis not feel daunting when the time comes :).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaybe I, Too, Am a Member of the Crustacean Cult\n\n\nReflecting on why I prefer R over Python for data-related tasks. TLDR: structure &gt; abstraction.\n\n\n\nJun 4, 2025\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Digital Garden"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Projects",
    "section": "",
    "text": "An Investigation of the Annals of Eugenics (1925-1953)\n\n\nExploring trends and patterns in the Annals of Eugenics.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Spending Habits With Logistic Regression\n\n\nExploring indicators of spending habit behaviors using logistic regression.\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting US Census Data with XGBoost\n\n\nUsing US Census data to predict whether or not someone has an annual income of more than $50,000.\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "assignments/job1/resume1.html",
    "href": "assignments/job1/resume1.html",
    "title": "Resume",
    "section": "",
    "text": "Back to top"
  }
]